{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb -qq\n",
    "import wandb\n",
    "import os\n",
    "import glob\n",
    "from gym.wrappers import Monitor\n",
    "import io\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classvariable():\n",
    "    def __init__(self):\n",
    "        self.lr = 0.00025\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 1.00\n",
    "        self.seed = 1\n",
    "        self.workers = 16\n",
    "        self.num_steps = 20\n",
    "        self.max_episode_length = 10000\n",
    "        self.env = 'SpaceInvaders-v0'\n",
    "        self.shared_optimizer = True\n",
    "        self.load = True\n",
    "        self.save_max = True\n",
    "        self.optimizer = 'RMSprop'\n",
    "        self.load_model_dir = '/content/drive/My Drive/Projects/A3C_GPU/trained_models/'\n",
    "        self.save_model_dir = '/content/drive/My Drive/Projects/A3C_GPU/trained_models/'\n",
    "        self.log_dir = '/content/drive/My Drive/Projects/A3C_GPU/logs/'\n",
    "        self.gpu_ids = [0]\n",
    "        self.amsgrad = True\n",
    "        self.skip_rate = 4\n",
    "\n",
    "args = classvariable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "#from skimage.color import rgb2gray\n",
    "from cv2 import resize\n",
    "#from skimage.transform import resize\n",
    "#from scipy.misc import imresize as resize\n",
    "import random\n",
    "\n",
    "\n",
    "def atari_env(env_id, env_conf, args):\n",
    "    env = gym.make(env_id)\n",
    "    if 'NoFrameskip' in env_id:\n",
    "        assert 'NoFrameskip' in env.spec.id\n",
    "        env._max_episode_steps = args.max_episode_length * args.skip_rate\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=args.skip_rate)\n",
    "    else:\n",
    "        env._max_episode_steps = args.max_episode_length\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env._max_episode_steps = args.max_episode_length\n",
    "    env = AtariRescale(env, env_conf)\n",
    "    env = NormalizedEnv(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def process_frame(frame, conf):\n",
    "    frame = frame[conf[\"crop1\"]:conf[\"crop2\"] + 160, :160]\n",
    "    frame = frame.mean(2)\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame *= (1.0 / 255.0)\n",
    "    frame = resize(frame, (80, conf[\"dimension2\"]))\n",
    "    frame = resize(frame, (80, 80))\n",
    "    frame = np.reshape(frame, [1, 80, 80])\n",
    "    return frame\n",
    "\n",
    "\n",
    "class AtariRescale(gym.ObservationWrapper):\n",
    "    def __init__(self, env, env_conf):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = Box(0.0, 1.0, [1, 80, 80], dtype=np.uint8)\n",
    "        self.conf = env_conf\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return process_frame(observation, self.conf)\n",
    "\n",
    "\n",
    "class NormalizedEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.num_steps += 1\n",
    "        self.state_mean = self.state_mean * self.alpha + \\\n",
    "            observation.mean() * (1 - self.alpha)\n",
    "        self.state_std = self.state_std * self.alpha + \\\n",
    "            observation.std() * (1 - self.alpha)\n",
    "\n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, self.was_real_done\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=3)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "def setup_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(message)s')\n",
    "    fileHandler = logging.FileHandler(log_file, mode='w')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    l.addHandler(streamHandler)\n",
    "\n",
    "\n",
    "def read_config(file_path):\n",
    "    \"\"\"Read JSON config.\"\"\"\n",
    "    json_object = json.load(open(file_path, 'r'))\n",
    "    return json_object\n",
    "\n",
    "\n",
    "def norm_col_init(weights, std=1.0):\n",
    "    x = torch.randn(weights.size())\n",
    "    x *= std / torch.sqrt((x**2).sum(1, keepdim=True))\n",
    "    return x\n",
    "\n",
    "\n",
    "def ensure_shared_grads(model, shared_model, gpu=False):\n",
    "    for param, shared_param in zip(model.parameters(),\n",
    "                                   shared_model.parameters()):\n",
    "        if shared_param.grad is not None and not gpu:\n",
    "            return\n",
    "        elif not gpu:\n",
    "            shared_param._grad = param.grad\n",
    "        else:\n",
    "            shared_param._grad = param.grad.cpu()\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class A3Clstm(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(A3Clstm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 5, stride=1, padding=2)\n",
    "        self.maxp1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, stride=1, padding=1)\n",
    "        self.maxp2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 4, stride=1, padding=1)\n",
    "        self.maxp3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.maxp4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(1024, 512)\n",
    "        num_outputs = action_space.n\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_outputs)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        relu_gain = nn.init.calculate_gain('relu')\n",
    "        #print(\"Enter A3C\")\n",
    "        self.conv1.weight.data.mul_(relu_gain)\n",
    "        #print(\"Conv1\")\n",
    "        self.conv2.weight.data.mul_(relu_gain)\n",
    "        #print(\"Conv2\")\n",
    "        self.conv3.weight.data.mul_(relu_gain)\n",
    "        #print(\"Conv3\")\n",
    "        self.conv4.weight.data.mul_(relu_gain)\n",
    "        #print(\"Conv4\")\n",
    "        self.actor_linear.weight.data = norm_col_init(\n",
    "            self.actor_linear.weight.data, 0.01)\n",
    "        #print(\"Actor_linear\")\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = norm_col_init(\n",
    "            self.critic_linear.weight.data, 1.0)\n",
    "        #print(\"Critic_linear\")\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "        #print(\"Exit A3C\")\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.relu(self.maxp1(self.conv1(inputs)))\n",
    "        x = F.relu(self.maxp2(self.conv2(x)))\n",
    "        x = F.relu(self.maxp3(self.conv3(x)))\n",
    "        x = F.relu(self.maxp4(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "\n",
    "        x = hx\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, model, env, args, state):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.state = state\n",
    "        self.hx = None\n",
    "        self.cx = None\n",
    "        self.eps_len = 0\n",
    "        self.args = args\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        self.done = True\n",
    "        self.info = None\n",
    "        self.reward = 0\n",
    "        self.gpu_id = -1\n",
    "\n",
    "    def action_train(self):\n",
    "        value, logit, (self.hx, self.cx) = self.model((Variable(\n",
    "            self.state.unsqueeze(0)), (self.hx, self.cx)))\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "        log_prob = F.log_softmax(logit, dim=1)\n",
    "        entropy = -(log_prob * prob).sum(1)\n",
    "        self.entropies.append(entropy)\n",
    "        action = prob.multinomial(1).data\n",
    "        log_prob = log_prob.gather(1, Variable(action))\n",
    "        state, self.reward, self.done, self.info = self.env.step(\n",
    "            action.cpu().numpy())\n",
    "        self.state = torch.from_numpy(state).float()\n",
    "        if self.gpu_id >= 0:\n",
    "            with torch.cuda.device(self.gpu_id):\n",
    "                self.state = self.state.cuda()\n",
    "        self.reward = max(min(self.reward, 1), -1)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(self.reward)\n",
    "        return self\n",
    "\n",
    "    def action_test(self):\n",
    "        with torch.no_grad():\n",
    "            if self.done:\n",
    "                if self.gpu_id >= 0:\n",
    "                    with torch.cuda.device(self.gpu_id):\n",
    "                        self.cx = Variable(\n",
    "                            torch.zeros(1, 512).cuda())\n",
    "                        self.hx = Variable(\n",
    "                            torch.zeros(1, 512).cuda())\n",
    "                else:\n",
    "                    self.cx = Variable(torch.zeros(1, 512))\n",
    "                    self.hx = Variable(torch.zeros(1, 512))\n",
    "            else:\n",
    "                self.cx = Variable(self.cx.data)\n",
    "                self.hx = Variable(self.hx.data)\n",
    "            value, logit, (self.hx, self.cx) = self.model((Variable(\n",
    "                self.state.unsqueeze(0)), (self.hx, self.cx)))\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "        action = prob.max(1)[1].data.cpu().numpy()\n",
    "        state, self.reward, self.done, self.info = self.env.step(action[0])\n",
    "        self.state = torch.from_numpy(state).float()\n",
    "        if self.gpu_id >= 0:\n",
    "            with torch.cuda.device(self.gpu_id):\n",
    "                self.state = self.state.cuda()\n",
    "        self.eps_len += 1\n",
    "        return self\n",
    "\n",
    "    def clear_actions(self):\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setproctitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class SharedRMSprop(optim.Optimizer):\n",
    "    \"\"\"Implements RMSprop algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=7e-4,\n",
    "                 alpha=0.99,\n",
    "                 eps=0.1,\n",
    "                 weight_decay=0,\n",
    "                 momentum=0,\n",
    "                 centered=False):\n",
    "        defaults = defaultdict(\n",
    "            lr=lr,\n",
    "            alpha=alpha,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=momentum,\n",
    "            centered=centered)\n",
    "        super(SharedRMSprop, self).__init__(params, defaults)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['grad_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['square_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['momentum_buffer'] = p.data.new().resize_as_(\n",
    "                    p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['square_avg'].share_memory_()\n",
    "                state['step'].share_memory_()\n",
    "                state['grad_avg'].share_memory_()\n",
    "                state['momentum_buffer'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'RMSprop does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "\n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n",
    "                    avg = square_avg.addcmul(-1, grad_avg,\n",
    "                                             grad_avg).sqrt().add_(\n",
    "                                                 group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                if group['momentum'] > 0:\n",
    "                    buf = state['momentum_buffer']\n",
    "                    buf.mul_(group['momentum']).addcdiv_(grad, avg)\n",
    "                    p.data.add_(-group['lr'], buf)\n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SharedAdam(optim.Optimizer):\n",
    "    \"\"\"Implements Adam algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-3,\n",
    "                 weight_decay=0,\n",
    "                 amsgrad=False):\n",
    "        defaults = defaultdict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            amsgrad=amsgrad)\n",
    "        super(SharedAdam, self).__init__(params, defaults)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['max_exp_avg_sq'] = p.data.new().resize_as_(\n",
    "                    p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "                state['max_exp_avg_sq'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Adam does not support sparse gradients, please consider SparseAdam instead'\n",
    "                    )\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till\n",
    "                    # now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1**state['step'].item()\n",
    "                bias_correction2 = 1 - beta2**state['step'].item()\n",
    "                step_size = group['lr'] * \\\n",
    "                    math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "setup_json = {\n",
    "  \"Default\": {\n",
    "    \"crop1\": 34,\n",
    "    \"crop2\": 34,\n",
    "    \"dimension2\": 80\n",
    "  },\n",
    "  \"Asteroids\": {\n",
    "    \"crop1\": 16,\n",
    "    \"crop2\": 34,\n",
    "    \"dimension2\": 94\n",
    "  },\n",
    "  \"BeamRider\": {\n",
    "    \"crop1\": 20,\n",
    "    \"crop2\": 20,\n",
    "    \"dimension2\": 80\n",
    "  },\n",
    "  \"Breakout\": {\n",
    "    \"crop1\": 34,\n",
    "    \"crop2\": 34,\n",
    "    \"dimension2\": 80\n",
    "  },\n",
    "  \"Centipede\": {\n",
    "    \"crop1\": 36,\n",
    "    \"crop2\": 56,\n",
    "    \"dimension2\": 90\n",
    "  },\n",
    "  \"MsPacman\": {\n",
    "    \"crop1\": 2,\n",
    "    \"crop2\": 10,\n",
    "    \"dimension2\": 84\n",
    "  },\n",
    "  \"Pong\": {\n",
    "    \"crop1\": 34,\n",
    "    \"crop2\": 34,\n",
    "    \"dimension2\": 80\n",
    "  },\n",
    "  \"Seaquest\": {\n",
    "    \"crop1\": 30,\n",
    "    \"crop2\": 30,\n",
    "    \"dimension2\": 80\n",
    "  },\n",
    "  \"SpaceInvaders\": {\n",
    "    \"crop1\": 8,\n",
    "    \"crop2\": 36,\n",
    "    \"dimension2\": 94\n",
    "  },\n",
    "  \"VideoPinball\": {\n",
    "    \"crop1\": 42,\n",
    "    \"crop2\": 60,\n",
    "    \"dimension2\": 89\n",
    "  },\n",
    "  \"Qbert\": {\n",
    "    \"crop1\": 12,\n",
    "    \"crop2\": 40,\n",
    "    \"dimension2\": 94\n",
    "  },\n",
    "  \"Boxing\": {\n",
    "    \"crop1\": 30,\n",
    "    \"crop2\": 30,\n",
    "    \"dimension2\": 80\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_reward = 0\n",
    "episode = 0\n",
    "def evaluate(episodic_reward):\n",
    "  '''\n",
    "  Takes in the reward for an episode, calculates the cumulative_avg_reward\n",
    "    and logs it in wandb. If episode > 100, stops logging scores to wandb.\n",
    "    Called after playing each episode. See example below.\n",
    "\n",
    "  Arguments:\n",
    "    episodic_reward - reward received after playing current episode\n",
    "  '''\n",
    "  global episode\n",
    "  global cumulative_reward\n",
    "  episode += 1\n",
    "\n",
    "  # your models will be evaluated on 100-episode average reward\n",
    "  # therefore, we stop logging after 100 episodes\n",
    "  if (episode > 100):\n",
    "    print(\"Scores from episodes > 100 won't be logged in wandb.\")\n",
    "    return\n",
    "\n",
    "  # log total reward received in this episode to wandb\n",
    "  wandb.log({'episodic_reward': episodic_reward})\n",
    "\n",
    "  # add reward from this episode to cumulative_reward\n",
    "  cumulative_reward += episodic_reward\n",
    "\n",
    "  # calculate the cumulative_avg_reward\n",
    "  # this is the metric your models will be evaluated on\n",
    "  cumulative_avg_reward = cumulative_reward/episode\n",
    "\n",
    "  # log cumulative_avg_reward over all episodes played so far\n",
    "  wandb.log({'cumulative_avg_reward': cumulative_avg_reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"qualcomm\")\n",
    "wandb.config.episodes = 100\n",
    "wandb.config.batch_size = 32\n",
    "wandb.config.learning_rate = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from setproctitle import setproctitle as ptitle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "def test(args, shared_model, env_conf):\n",
    "    ptitle('Test Agent')\n",
    "    print('Test Agent')\n",
    "    gpu_id = args.gpu_ids[-1]\n",
    "    log = {}\n",
    "    setup_logger('{}_log'.format(args.env), r'{0}{1}_log'.format(\n",
    "        args.log_dir, args.env))\n",
    "    log['{}_log'.format(args.env)] = logging.getLogger('{}_log'.format(\n",
    "        args.env))\n",
    "    d_args = vars(args)\n",
    "    for k in d_args.keys():\n",
    "        log['{}_log'.format(args.env)].info('{0}: {1}'.format(k, d_args[k]))\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if gpu_id >= 0:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    env = atari_env(args.env, env_conf, args)\n",
    "    env = gym.wrappers.Monitor(env, './video', force=True)\n",
    "    reward_sum = 0\n",
    "    start_time = time.time()\n",
    "    num_tests = 0\n",
    "    reward_total_sum = 0\n",
    "    player = Agent(None, env, args, None)\n",
    "    player.gpu_id = gpu_id\n",
    "    player.model = A3Clstm(player.env.observation_space.shape[0],\n",
    "                           player.env.action_space)\n",
    "\n",
    "    player.state = player.env.reset()\n",
    "    player.eps_len += 2\n",
    "    player.state = torch.from_numpy(player.state).float()\n",
    "    if gpu_id >= 0:\n",
    "        with torch.cuda.device(gpu_id):\n",
    "            player.model = player.model.cuda()\n",
    "            player.state = player.state.cuda()\n",
    "    flag = True\n",
    "    max_score = 0\n",
    "    for episode in range(wandb.config.episodes):\n",
    "      while True:\n",
    "        if flag:\n",
    "            if gpu_id >= 0:\n",
    "                with torch.cuda.device(gpu_id):\n",
    "                    player.model.load_state_dict(shared_model.state_dict())\n",
    "            else:\n",
    "                player.model.load_state_dict(shared_model.state_dict())\n",
    "            player.model.eval()\n",
    "            flag = False\n",
    "\n",
    "        player.action_test()\n",
    "        reward_sum += player.reward\n",
    "\n",
    "        if player.done and not player.info:\n",
    "            state = player.env.reset()\n",
    "            player.eps_len += 2\n",
    "            player.state = torch.from_numpy(state).float()\n",
    "            if gpu_id >= 0:\n",
    "                with torch.cuda.device(gpu_id):\n",
    "                    player.state = player.state.cuda()\n",
    "        elif player.info:\n",
    "            flag = True\n",
    "            num_tests += 1\n",
    "            reward_total_sum += reward_sum\n",
    "            reward_mean = reward_total_sum / num_tests\n",
    "            log['{}_log'.format(args.env)].info(\n",
    "                \"episode {0}, episode reward {1}, episode length {2}, reward mean {3:.4f}\".\n",
    "                format(episode+1,\n",
    "                    reward_sum, player.eps_len, reward_mean))\n",
    "            evaluate(reward_sum)\n",
    "\n",
    "            if args.save_max and reward_sum >= max_score:\n",
    "                max_score = reward_sum\n",
    "                if gpu_id >= 0:\n",
    "                    with torch.cuda.device(gpu_id):\n",
    "                        state_to_save = player.model.state_dict()\n",
    "                        torch.save(state_to_save, os.path.join(wandb.run.dir, \"model.dat\"))\n",
    "                else:\n",
    "                    state_to_save = player.model.state_dict()\n",
    "                    torch.save(state_to_save, os.path.join(wandb.run.dir, \"model.dat\"))\n",
    "            if ((episode+1)%10) == 0:\n",
    "                mp4list = glob.glob('video/*.mp4')\n",
    "                if len(mp4list) > 0:\n",
    "                  mp4 = mp4list[-1]\n",
    "                  video = io.open(mp4, 'r+b').read()\n",
    "                  encoded = base64.b64encode(video)\n",
    "\n",
    "                  # log gameplay video in wandb\n",
    "                  print(\"Video being logged now\")\n",
    "                  wandb.log({\"gameplays\": wandb.Video(mp4, fps=4, format=\"gif\")})\n",
    "\n",
    "            reward_sum = 0\n",
    "            player.eps_len = 0\n",
    "            state = player.env.reset()\n",
    "            player.eps_len += 2\n",
    "            time.sleep(10)\n",
    "            player.state = torch.from_numpy(state).float()\n",
    "            if gpu_id >= 0:\n",
    "                with torch.cuda.device(gpu_id):\n",
    "                    player.state = player.state.cuda()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_conf = setup_json[\"Default\"]\n",
    "for i in setup_json.keys():\n",
    "  if i in args.env:\n",
    "    env_conf = setup_json[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = atari_env(args.env, env_conf, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3Clstm(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTMCell(1024, 512)\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (actor_linear): Linear(in_features=512, out_features=6, bias=True)\n",
      ")"
     ]
    }
   ],
   "source": [
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args.load:\n",
    "  saved_state = torch.load('{0}{1}.dat'.format(args.load_model_dir, args.env),\n",
    "  map_location=lambda storage, loc: storage)\n",
    "  shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(args,shared_model,env_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
